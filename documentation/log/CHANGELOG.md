# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/),
and this project adheres to [Semantic Versioning](https://semver.org/).

## [1.1.0] - 2026-02-11

### Added

#### Phase 1: Multi-Language Support

- **8 Language Support**: English, Indonesian, Spanish, French, German, Portuguese, Japanese, Mandarin Chinese
- **Dynamic Language Switching**: Change language via voice command at runtime
- **Language-Specific Speech Recognition**: Google Speech Recognition with language codes
- **Multi-Provider TTS**: Google TTS (online) + pyttsx3 (offline) for all languages
- **Command-Line Language Selection**: `python app.py --lang=id` for Indonesian
- **Configuration-Driven Management**: All languages configurable in `config.yaml`
- **Voice Feedback in Native Language**: All responses in user's selected language
- **Test Suite**: `test_multilingual.py` for validation

#### Phase 2: Intelligent Face Recognition with Training

- **Face Detection**: HOG (fast) and CNN (accurate) models
- **Face Recognition**: Compare detected faces against known faces
- **Face Enrollment/Training**: Learn new faces with voice commands ("Enroll John")
- **Multiple Samples Per Person**: Store up to 10 training samples per person
- **Persistent Face Database**: Automatic save/load with pickle serialization
- **Face Management**: List enrolled people, remove/forget people
- **Confidence Scoring**: Accuracy reporting for recognized faces
- **Database Models**: New `Person` and `FaceEncoding` ORM models
- **Voice Commands**: Enrollment, management, statistics
- **Test Suite**: `test_face_recognition.py` for validation

#### Phase 3: 3D Audio Localization & Obstacle Detection

- **Sound Detection**: Real-time audio event detection above noise floor
- **Sound Localization**: 8-directional compass output (Front, Right, Back, Front-Left, etc.)
- **Distance Estimation**: Estimates distance to sound sources (0.5-10m range)
- **Obstacle Detection**: Audio echo analysis for detecting barriers/obstacles
- **Automatic Warnings**: Alerts when obstacles detected within threshold (default 2m)
- **Sound Classification**: Identifies sound types (speech, alarms, doors, ambient)
- **Frequency Analysis**: STFT-based frequency domain analysis
- **Auto-Localization**: Beamforming and phase-shift methods
- **Navigation Integration**: Audio-guided directions using sound cues
- **Voice Commands**: "What do you hear?", "Check ahead", "Classify sound"
- **Test Suite**: `test_sound_localization.py` for validation

### Changed

- **NavigationAssistant**: Enhanced with audio guidance methods and sound integration
- **VisionAssistant**: Added SoundLocalizer initialization and audio command handlers
- **SpeechEngine**: Full YAML configuration support with language profiles
- **VisionAssistant.**init****: Now accepts optional `language` parameter
- **app.py**: Extended with `_handle_face_enrollment()`, `_handle_face_management()`, `_handle_audio_assistance()` methods
- **config.yaml**: Added `sound_localization` section with 40+ settings; enhanced `speech` section; enhanced `face_recognition` section

### New Files

- `ai_modules/sound_localization.py` - Complete audio processing and localization module
- `documentation/RELEASE_v1.1.md` - Official v1.1 release notes
- `documentation/VOICE_COMMANDS.md` - Comprehensive voice commands reference
- `documentation/CONFIG_GUIDE.md` - Detailed configuration guide
- `test_multilingual.py` - Multi-language feature tests
- `test_face_recognition.py` - Face recognition feature tests
- `test_sound_localization.py` - Audio localization feature tests

### Documentation

- **Voice Commands Reference**: 100+ categorized commands with examples
- **Configuration Guide**: Detailed settings for all 3 features
- **Architecture Documentation**: Updated system design docs
- **Troubleshooting Guide**: Common issues and solutions
- **Release Notes**: Complete v1.1 feature overview

### Dependencies (New)

- `librosa>=0.10.0` - Audio analysis and feature extraction
- `scipy>=1.10.0` - Signal processing utilities
- Enhanced `pyaudio` support for audio input

### Performance Improvements

- Optimized audio chunk processing (~100ms detection)
- Efficient face encoding lookup (<50ms per match)
- Streamlined language switching (<500ms)
- Reduced memory footprint for audio processing

### Infrastructure

- Enhanced configuration system with YAML validation
- Improved error handling for audio/hardware issues
- Better logging for debugging
- Cross-platform audio support

---

## [1.0.0] - 2026-02-09

### Added

#### Core Features

- **Vision Processing Pipeline**
  - Object detection using YOLOv8 (nano model)
  - Text extraction using EasyOCR
  - Face detection using OpenCV Haar cascades
  - Scene description with context awareness
  - Detailed vs. brief environment analysis

- **Voice Interaction System**
  - Speech-to-text using Google Speech Recognition
  - Text-to-speech with dual providers (pyttsx3 offline + gTTS online)
  - Multi-language support (configurable)
  - Adjustable speech rate (50-300 WPM)
  - Voice feedback for all operations

- **Language Model Integration**
  - OpenAI GPT integration for intelligent responses
  - Local keyword-based intent recognition fallback
  - Intent classification and parameter extraction
  - Context-aware response generation
  - 8 core intent types (describe_scene, read_text, etc.)

- **Navigation Assistance**
  - Current location retrieval
  - Route planning and directions
  - Nearby location discovery
  - GPS-based assistance

- **Database Persistence**
  - SQLAlchemy ORM with SQLite backend
  - User preference storage
  - Conversation history tracking
  - Scene memory for location context
  - Object detection history

- **Configuration System**
  - YAML-based application configuration
  - Environment variable support (.env)
  - Runtime parameter override capability
  - Secure API key management

#### Exit Command Recognition

- Recognizes 8+ goodbye variations: "goodbye", "bye", "exit", "quit", "stop", "turn off", "shut down", "close"
- Graceful shutdown with resource cleanup
- Voice confirmation before exit

#### Accessibility Features

- Voice-first interface (no visual UI required)
- Audio-only operation for complete accessibility
- Customizable speech rate and language
- Offline-capable core features

### Infrastructure

#### Environment Setup

- Python 3.8+ support
- Virtual environment configuration
- Automated dependency management
- Cross-platform support (Windows, macOS, Linux)

#### Dependencies (Core)

- PyTorch 2.0.1+ (deep learning framework)
- Transformers 4.31.0+ (NLP models)
- YOLOv8 (ultralytics 8.0.124+)
- OpenCV 4.8.0+
- EasyOCR 1.7.0+
- SpeechRecognition 3.10.0+
- pyttsx3 2.90+ (offline TTS)
- gTTS 2.3.2+ (online TTS)
- FastAPI (backend framework)
- SQLAlchemy 2.0.19+ (ORM)
- OpenAI 1.0.0+ (LLM API)

#### Development Tools

- Black (code formatting)
- Flake8 (linting)
- mypy (type checking)
- pytest (testing framework)

### Documentation

#### User Documentation

- **README.md** - Project overview, features, quick start, troubleshooting
- **INSTALLATION.md** - Platform-specific setup guide with system requirements
- **API.md** - Complete API reference with code examples
- **ARCHITECTURE.md** - System design and module relationships
- **CONTRIBUTING.md** - Development guidelines and contribution workflow
- **LICENSE** - MIT License

#### Internal Documentation

- `.github/copilot-instructions.md` - Agent onboarding guide
- Inline code documentation with docstrings
- Configuration examples (.env.example)

### Initial File Structure

```
ai_modules/
├── vision_processor.py
├── speech_engine.py
├── llm_handler.py
├── neural_core.py
└── __init__.py

features/
├── navigation.py
├── object_detection.py
├── text_reader.py
├── face_recognition.py
└── __init__.py

database/
├── models.py (ORM models)
├── db_handler.py
└── __init__.py

api_integration/
└── __init__.py

tests/
└── validation.py

app.py (main application)
config.yaml (configuration)
requirements.txt (dependencies)
.env.example (environment template)
README.md
INSTALLATION.md
API.md
ARCHITECTURE.md
CONTRIBUTING.md
LICENSE
```

### Fixed Issues

#### Error Resolution

1. **Missing ultralytics package** - Added to requirements.txt
2. **image-to-text pipeline deprecated** - Replaced with manual inference
3. **Speech rate attribute error** - Fixed initialization order
4. **Database schema mismatch** - Updated models with required fields
5. **OpenAI API deprecation** - Migrated to new client-based API (1.0.0+)

### Known Limitations

- Advanced face recognition (identification) uses placeholder implementation
- Cloud integrations (Google Maps, Weather) are optional
- Mobile deployment requires optimization
- Model download required on first run (~1GB)
- Requires camera and microphone for full functionality

### Performance Metrics

- **Startup Time**: 2-3 seconds after initialization
- **Object Detection**: 100-200ms per image (CPU)
- **Voice Recognition**: 1-2 seconds
- **Response Generation**: <2 seconds average
- **Memory Usage**: <500MB idle, <1GB during processing

### Platform Support

- **Windows 10+** - Full support
- **macOS 10.15+** - Full support
- **Linux (Ubuntu 18.04+)** - Full support
- **iOS/Android** - Planned for future release

### Security

- API keys stored in .env (not committed)
- HTTPS for all external API calls
- Local data storage option
- No automatic cloud sync without user consent

---

## [0.9.0] - Pre-Release (Unreleased)

### Initial Development

- Project structure established
- Core modules skeleton
- Async architecture design
- Configuration framework

---

## Future Roadmap

### [1.1.0] - Planned

#### Features

- Advanced face recognition and identification
- Real-time scene understanding with AI
- Multi-user support with profiles
- Conversation context persistence
- Emotion detection from voice

#### Improvements

- Mobile app deployment (iOS/Android)
- Model quantization for faster inference
- Battery optimization for mobile
- Improved offline capabilities

#### Infrastructure

- Docker containerization
- Cloud deployment option
- CI/CD pipeline with GitHub Actions
- Automated testing suite

### [1.2.0] - Planned

#### Accessibility Enhancements

- Braille output support
- Multiple TTS voice options
- Haptic feedback (mobile)
- Customizable audio profiles

#### Features

- Document scanning and reading
- Color detection and description
- Receipt and bill reading
- Barcode/QR code scanning

### [2.0.0] - Future

#### Major Features

- Multi-modal input (voice + touch)
- Real-time video processing
- Advanced neural networks
- Edge computing optimization

#### Deployment

- Full cloud architecture
- Federated learning
- Cross-platform synchronization
- Enterprise features

---

## Contributing

See [CONTRIBUTING.md](../CONTRIBUTING.md) for details on how to contribute to this changelog.

### Guidelines for Changes

- Use present tense ("Add feature" not "Added feature")
- Reference issues and pull requests liberally
- Group changes by category (Added, Changed, Fixed, Removed, etc.)
- Maintain semantic versioning

---

## Versioning

This project uses [Semantic Versioning](https://semver.org/):

- **MAJOR** version for incompatible API changes
- **MINOR** version for new functionality (backwards compatible)
- **PATCH** version for bug fixes

---

## See Also

- [README.md](/README.md) - Project overview
- [CONTRIBUTING.md](../guidelines/CONTRIBUTING.md) - How to contribute
- [API.md](../API.md) - API reference
- [ARCHITECTURE.md](../ARCHITECTURE.md) - System design
